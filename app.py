# -*- coding: utf-8 -*-
"""AIPRODUCT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14TFkcogH39g-tEAATt_-iZVMdjmn0f6G
"""

!pip -q install pdfplumber==0.11.4 python-docx==1.1.2 beautifulsoup4==4.12.3 lxml==5.3.0
# scikit-learn is already in Colab; we’ll use the built-in one.

import os, io, json, uuid, re
from typing import List, Dict, Tuple

import pdfplumber
from docx import Document as DocxDocument
from bs4 import BeautifulSoup

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

BASE_DIR = "/content/knowledge_mvp_tfidf"
os.makedirs(BASE_DIR, exist_ok=True)

CHUNK_WORDS   = 800
CHUNK_OVERLAP = 150
TOP_K_CHUNKS  = 6
TOP_SENTENCES = 4

SENTENCE_SPLIT_RE = re.compile(r'(?<=[.!?])\s+(?=[A-Z0-9“"(])')

def read_pdf(file_bytes: bytes) -> List[Tuple[str, int]]:
    out = []
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            text = page.extract_text() or ""
            out.append((text, i))
    return out

def read_docx(file_bytes: bytes) -> str:
    doc = DocxDocument(io.BytesIO(file_bytes))
    return "\n".join(p.text for p in doc.paragraphs)

def read_txt(file_bytes: bytes) -> str:
    try:    return file_bytes.decode("utf-8")
    except: return file_bytes.decode("latin-1", errors="ignore")

def clean_text(s: str) -> str:
    s = s.replace("\r", "\n")
    s = BeautifulSoup(s, "lxml").get_text()
    lines = [ln.strip() for ln in s.splitlines()]
    return "\n".join([ln for ln in lines if ln])

def chunk_text(text: str, size_words=CHUNK_WORDS, overlap=CHUNK_OVERLAP):
    if not text: return []
    words, chunks, start = text.split(), [], 0
    while start < len(words):
        end = min(len(words), start + size_words)
        chunks.append(" ".join(words[start:end]))
        if end == len(words): break
        start = max(0, end - overlap)
    return chunks

def split_sentences(text: str) -> List[str]:
    sentences = []
    for para in text.split("\n"):
        para = para.strip()
        if not para: continue
        parts = SENTENCE_SPLIT_RE.split(para)
        for s in parts:
            s = s.strip()
            if s: sentences.append(s)
    return sentences

class Store:
    def __init__(self, workspace: str = "default"):
        self.workspace = workspace
        self.dir = os.path.join(BASE_DIR, workspace)
        os.makedirs(self.dir, exist_ok=True)
        self.meta_path = os.path.join(self.dir, "meta.json")

        self.meta = {"docs": [], "chunks": []}
        self.vectorizer: TfidfVectorizer | None = None
        self.matrix = None
        self._load()

    def _load(self):
        if os.path.exists(self.meta_path):
            with open(self.meta_path, "r", encoding="utf-8") as f:
                self.meta = json.load(f)
        if self.meta["chunks"]:
            self._rebuild_index()

    def _save(self):
        with open(self.meta_path, "w", encoding="utf-8") as f:
            json.dump(self.meta, f, ensure_ascii=False, indent=2)

    def reset(self):
        for fn in os.listdir(self.dir):
            try: os.remove(os.path.join(self.dir, fn))
            except: pass
        self.meta = {"docs": [], "chunks": []}
        self.vectorizer = None
        self.matrix = None

    def _rebuild_index(self):
        texts = [c["text"] for c in self.meta["chunks"]]
        self.vectorizer = TfidfVectorizer(
            lowercase=True, stop_words="english", ngram_range=(1,2), max_features=100_000, norm="l2"
        )
        self.matrix = self.vectorizer.fit_transform(texts)

    def add(self, filename: str, file_bytes: bytes) -> int:
        ext = filename.lower().split(".")[-1]
        entries = []
        if ext == "pdf":
            for page_text, page_no in read_pdf(file_bytes):
                text = clean_text(page_text)
                for ch in chunk_text(text):
                    entries.append({"id": str(uuid.uuid4()), "filename": filename, "page": page_no, "text": ch})
        elif ext == "docx":
            text = clean_text(read_docx(file_bytes))
            for ch in chunk_text(text):
                entries.append({"id": str(uuid.uuid4()), "filename": filename, "page": None, "text": ch})
        elif ext == "txt":
            text = clean_text(read_txt(file_bytes))
            for ch in chunk_text(text):
                entries.append({"id": str(uuid.uuid4()), "filename": filename, "page": None, "text": ch})
        else:
            raise ValueError(f"Unsupported file type: .{ext}")

        if not entries: return 0
        self.meta["chunks"].extend(entries)
        self.meta["docs"].append({"filename": filename, "size": len(file_bytes)})
        self._save()
        self._rebuild_index()
        return len(entries)

    def search(self, query: str, top_k: int = TOP_K_CHUNKS):
        if not self.vectorizer or self.matrix is None or not self.meta["chunks"]:
            return []
        qv = self.vectorizer.transform([query])
        sims = cosine_similarity(qv, self.matrix)[0]
        top_idx = np.argsort(-sims)[:top_k]
        results = []
        for idx in top_idx:
            ch = self.meta["chunks"][int(idx)]
            results.append({"score": float(sims[int(idx)]), "chunk": ch})
        return results
class Store:
    def __init__(self, workspace: str = "default"):
        self.workspace = workspace
        self.dir = os.path.join(BASE_DIR, workspace)
        os.makedirs(self.dir, exist_ok=True)
        self.meta_path = os.path.join(self.dir, "meta.json")

        self.meta = {"docs": [], "chunks": []}
        self.vectorizer: TfidfVectorizer | None = None
        self.matrix = None
        self._load()

    def _load(self):
        if os.path.exists(self.meta_path):
            with open(self.meta_path, "r", encoding="utf-8") as f:
                self.meta = json.load(f)
        if self.meta["chunks"]:
            self._rebuild_index()

    def _save(self):
        with open(self.meta_path, "w", encoding="utf-8") as f:
            json.dump(self.meta, f, ensure_ascii=False, indent=2)

    def reset(self):
        for fn in os.listdir(self.dir):
            try: os.remove(os.path.join(self.dir, fn))
            except: pass
        self.meta = {"docs": [], "chunks": []}
        self.vectorizer = None
        self.matrix = None

    def _rebuild_index(self):
        texts = [c["text"] for c in self.meta["chunks"]]
        self.vectorizer = TfidfVectorizer(
            lowercase=True, stop_words="english", ngram_range=(1,2), max_features=100_000, norm="l2"
        )
        self.matrix = self.vectorizer.fit_transform(texts)

    def add(self, filename: str, file_bytes: bytes) -> int:
        ext = filename.lower().split(".")[-1]
        entries = []
        if ext == "pdf":
            for page_text, page_no in read_pdf(file_bytes):
                text = clean_text(page_text)
                for ch in chunk_text(text):
                    entries.append({"id": str(uuid.uuid4()), "filename": filename, "page": page_no, "text": ch})
        elif ext == "docx":
            text = clean_text(read_docx(file_bytes))
            for ch in chunk_text(text):
                entries.append({"id": str(uuid.uuid4()), "filename": filename, "page": None, "text": ch})
        elif ext == "txt":
            text = clean_text(read_txt(file_bytes))
            for ch in chunk_text(text):
                entries.append({"id": str(uuid.uuid4()), "filename": filename, "page": None, "text": ch})
        else:
            raise ValueError(f"Unsupported file type: .{ext}")

        if not entries: return 0
        self.meta["chunks"].extend(entries)
        self.meta["docs"].append({"filename": filename, "size": len(file_bytes)})
        self._save()
        self._rebuild_index()
        return len(entries)

    def search(self, query: str, top_k: int = TOP_K_CHUNKS):
        if not self.vectorizer or self.matrix is None or not self.meta["chunks"]:
            return []
        qv = self.vectorizer.transform([query])
        sims = cosine_similarity(qv, self.matrix)[0]
        top_idx = np.argsort(-sims)[:top_k]
        results = []
        for idx in top_idx:
            ch = self.meta["chunks"][int(idx)]
            results.append({"score": float(sims[int(idx)]), "chunk": ch})
        return results

def answer_with_sentences(question: str, retrieved: List[Dict], top_sentences: int = TOP_SENTENCES) -> Dict:
    if not retrieved:
        return {"answer": "No documents indexed yet or no relevant passages found.", "citations": []}

    sentences = []
    for r in retrieved:
        ch = r["chunk"]
        for s in split_sentences(ch["text"]):
            if len(s) >= 5:
                sentences.append({"sentence": s, "filename": ch["filename"], "page": ch.get("page")})

    if not sentences:
        return {"answer": "I couldn't extract useful sentences from the retrieved passages.", "citations": []}

    vec = TfidfVectorizer(lowercase=True, stop_words="english", ngram_range=(1,2), norm="l2")
    sent_matrix = vec.fit_transform([s["sentence"] for s in sentences])
    q_vec = vec.transform([question])
    sims = cosine_similarity(q_vec, sent_matrix)[0]

    order = np.argsort(-sims)
    picks, seen = [], set()
    for idx in order:
        s = sentences[idx]
        key = s["sentence"].lower()[:200]
        if key in seen:
            continue
        seen.add(key)
        picks.append((float(sims[idx]), s))
        if len(picks) >= top_sentences:
            break

    answer_text = " ".join([p[1]["sentence"] for p in picks])
    citations = [{
        "filename": p[1]["filename"],
        "page": p[1]["page"],
        "score": p[0],
        "preview": p[1]["sentence"][:200] + ("..." if len(p[1]["sentence"]) > 200 else "")
    } for p in picks]

    return {"answer": answer_text, "citations": citations}

!pip install reportlab

from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas

pdf_path = "/content/sample_contract.pdf"

c = canvas.Canvas(pdf_path, pagesize=letter)
width, height = letter

c.setFont("Helvetica", 12)
c.drawString(72, height - 72, "ACME SaaS Agreement")
c.drawString(72, height - 100, "This Agreement is entered into on 1 Jan 2024.")
c.drawString(72, height - 120, "The service will automatically renew on 31 Dec 2025.")
c.drawString(72, height - 140, "The notice period for termination is 30 days before renewal.")
c.drawString(72, height - 160, "Early termination is allowed for material breach.")

c.save()
print("PDF created at:", pdf_path)

store = Store("default")
with open("/content/sample_contract.pdf", "rb") as f:
    store.add("sample_contract.pdf", f.read())
print("Sample PDF indexed.")

def ask(question: str, top_k: int = TOP_K_CHUNKS, top_sents: int = TOP_SENTENCES):
    results = store.search(question, top_k=top_k)
    out

!pip -q install streamlit==1.38.0 pdfplumber==0.11.4 python-docx==1.1.2 beautifulsoup4==4.12.3 lxml==5.3.0 scikit-learn==1.5.2 reportlab==4.2.2
!pip -q install cloudflared

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import os
# import io
# import re
# import json
# import uuid
# from pathlib import Path
# from typing import List, Dict, Tuple
# 
# import streamlit as st
# from bs4 import BeautifulSoup
# import pdfplumber
# from docx import Document as DocxDocument
# 
# import numpy as np
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity
# 
# # PDF export
# from reportlab.lib.pagesizes import letter
# from reportlab.pdfgen import canvas
# from reportlab.lib.units import inch
# 
# # ----------------------------
# # Config (DATA_DIR can be overridden via env)
# # ----------------------------
# APP_TITLE = "Knowledge Assistant (MVP)"
# DATA_DIR = Path(os.getenv("DATA_DIR", "./data"))
# DATA_DIR.mkdir(exist_ok=True, parents=True)
# 
# CHUNK_WORDS   = 800
# CHUNK_OVERLAP = 150
# TOP_K_CHUNKS  = 6
# TOP_SENTENCES = 4
# SENTENCE_SPLIT_RE = re.compile(r'(?<=[.!?])\s+(?=[A-Z0-9“"(])')
# 
# # ✅ IMPORTANT: page config MUST be the first Streamlit command
# st.set_page_config(page_title=APP_TITLE, layout="wide")
# 
# # ----------------------------
# # Auth gate (single password)
# # ----------------------------
# def check_auth():
#     required = os.getenv("APP_PASSWORD")
#     if not required:
#         return True
#     if "authed" not in st.session_state:
#         st.session_state.authed = False
#     if st.session_state.authed:
#         return True
#     st.title(APP_TITLE)
#     st.info("Enter password to continue.")
#     pwd = st.text_input("Password", type="password")
#     if st.button("Let me in"):
#         if pwd == required:
#             st.session_state.authed = True
#             return True
#         else:
#             st.error("Wrong password.")
#     st.stop()
# 
# # ----------------------------
# # Readers & utilities
# # ----------------------------
# def read_pdf(file_bytes: bytes) -> List[Tuple[str, int]]:
#     out = []
#     with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
#         for i, page in enumerate(pdf.pages, start=1):
#             text = page.extract_text() or ""
#             out.append((text, i))
#     return out
# 
# def read_docx(file_bytes: bytes) -> str:
#     doc = DocxDocument(io.BytesIO(file_bytes))
#     return "\n".join(p.text for p in doc.paragraphs)
# 
# def read_txt(file_bytes: bytes) -> str:
#     try:
#         return file_bytes.decode("utf-8")
#     except UnicodeDecodeError:
#         return file_bytes.decode("latin-1", errors="ignore")
# 
# def clean_text(s: str) -> str:
#     s = s.replace("\r", "\n")
#     s = BeautifulSoup(s, "lxml").get_text()
#     lines = [ln.strip() for ln in s.splitlines()]
#     return "\n".join([ln for ln in lines if ln])
# 
# def chunk_text(text: str, size_words=CHUNK_WORDS, overlap=CHUNK_OVERLAP):
#     if not text: return []
#     words, chunks, start = text.split(), [], 0
#     while start < len(words):
#         end = min(len(words), start + size_words)
#         chunks.append(" ".join(words[start:end]))
#         if end == len(words): break
#         start = max(0, end - overlap)
#     return chunks
# 
# def split_sentences(text: str) -> List[str]:
#     sentences = []
#     for para in text.split("\n"):
#         para = para.strip()
#         if not para: continue
#         parts = SENTENCE_SPLIT_RE.split(para)
#         for s in parts:
#             s = s.strip()
#             if s: sentences.append(s)
#     return sentences
# 
# # ----------------------------
# # TF-IDF store (per workspace)
# # ----------------------------
# class Store:
#     def __init__(self, workspace: str):
#         self.workspace = workspace
#         self.dir = DATA_DIR / workspace
#         self.dir.mkdir(exist_ok=True, parents=True)
#         self.meta_path = self.dir / "meta.json"
#         self.meta = {"docs": [], "chunks": []}
#         self.vectorizer: TfidfVectorizer | None = None
#         self.matrix = None
#         self._load()
# 
#     def _load(self):
#         if self.meta_path.exists():
#             self.meta = json.loads(self.meta_path.read_text(encoding="utf-8"))
#         if self.meta["chunks"]:
#             self._rebuild_index()
# 
#     def _save(self):
#         self.meta_path.write_text(json.dumps(self.meta, ensure_ascii=False, indent=2), encoding="utf-8")
# 
#     def reset(self):
#         for p in self.dir.glob("*"):
#             try: p.unlink()
#             except: pass
#         self.meta = {"docs": [], "chunks": []}
#         self.vectorizer = None
#         self.matrix = None
# 
#     def _rebuild_index(self):
#         texts = [c["text"] for c in self.meta["chunks"]]
#         self.vectorizer = TfidfVectorizer(
#             lowercase=True, stop_words="english", ngram_range=(1,2),
#             max_features=100_000, norm="l2"
#         )
#         self.matrix = self.vectorizer.fit_transform(texts)
# 
#     def add(self, filename: str, file_bytes: bytes) -> int:
#         ext = filename.lower().split(".")[-1]
#         entries = []
#         if ext == "pdf":
#             for page_text, page_no in read_pdf(file_bytes):
#                 text = clean_text(page_text)
#                 for ch in chunk_text(text):
#                     entries.append({"id": str(uuid.uuid4()), "filename": filename, "page": page_no, "text": ch})
#         elif ext == "docx":
#             text = clean_text(read_docx(file_bytes))
#             for ch in chunk_text(text):
#                 entries.append({"id": str(uuid.uuid4()), "filename": filename, "page": None, "text": ch})
#         elif ext == "txt":
#             text = clean_text(read_txt(file_bytes))
#             for ch in chunk_text(text):
#                 entries.append({"id": str(uuid.uuid4()), "filename": filename, "page": None, "text": ch})
#         else:
#             raise ValueError(f"Unsupported file type: .{ext}")
#         if not entries: return 0
#         self.meta["chunks"].extend(entries)
#         self.meta["docs"].append({"filename": filename, "size": len(file_bytes)})
#         self._save()
#         self._rebuild_index()
#         return len(entries)
# 
#     def search(self, query: str, top_k: int = TOP_K_CHUNKS):
#         if not self.vectorizer or self.matrix is None or not self.meta["chunks"]:
#             return []
#         qv = self.vectorizer.transform([query])
#         sims = cosine_similarity(qv, self.matrix)[0]
#         top_idx = np.argsort(-sims)[:top_k]
#         return [{"score": float(sims[i]), "chunk": self.meta["chunks"][i]} for i in top_idx]
# 
# # ----------------------------
# # Answer composer (top sentences)
# # ----------------------------
# def answer_with_sentences(question: str, retrieved: List[Dict], top_sentences: int = TOP_SENTENCES) -> Dict:
#     if not retrieved:
#         return {"answer": "No documents indexed yet or no relevant passages found.", "citations": []}
#     sentences = []
#     for r in retrieved:
#         ch = r["chunk"]
#         for s in split_sentences(ch["text"]):
#             if len(s) >= 5:
#                 sentences.append({"sentence": s, "filename": ch["filename"], "page": ch.get("page")})
#     if not sentences:
#         return {"answer": "I couldn't extract meaningful sentences from the retrieved passages.", "citations": []}
#     vec = TfidfVectorizer(lowercase=True, stop_words="english", ngram_range=(1,2), norm="l2")
#     sent_matrix = vec.fit_transform([s["sentence"] for s in sentences])
#     q_vec = vec.transform([question])
#     sims = cosine_similarity(q_vec, sent_matrix)[0]
#     order = np.argsort(-sims)
#     picks, seen = [], set()
#     for idx in order:
#         s = sentences[idx]
#         key = s["sentence"].lower()[:200]
#         if key in seen:
#             continue
#         seen.add(key)
#         picks.append((float(sims[idx]), s))
#         if len(picks) >= top_sentences:
#             break
#     answer_text = " ".join([p[1]["sentence"] for p in picks])
#     citations = [{
#         "filename": p[1]["filename"],
#         "page": p[1]["page"],
#         "score": p[0],
#         "preview": p[1]["sentence"][:200] + ("..." if len(p[1]["sentence"]) > 200 else "")
#     } for p in picks]
#     return {"answer": answer_text, "citations": citations}
# 
# # ----------------------------
# # UI
# # ----------------------------
# def main():
#     check_auth()
# 
#     st.title(APP_TITLE)
# 
#     # Workspace
#     st.sidebar.header("Workspace")
#     existing = sorted([p.name for p in DATA_DIR.iterdir() if p.is_dir()])
#     ws = st.sidebar.text_input("Name", value=(existing[0] if existing else "default"))
#     if st.sidebar.button("Create/Use"):
#         st.session_state.workspace = ws
#     if "workspace" not in st.session_state:
#         st.session_state.workspace = ws
#     ws = st.session_state.workspace
#     st.sidebar.write(f"Active: **{ws}**")
# 
#     store = Store(ws)
# 
#     if st.sidebar.button("Reset workspace"):
#         store.reset()
#         st.success("Workspace reset. (All files & index cleared)")
# 
#     st.subheader("1) Upload documents")
#     files = st.file_uploader("Add PDF / DOCX / TXT", type=["pdf", "docx", "txt"], accept_multiple_files=True)
#     if st.button("Ingest & Index") and files:
#         total = 0
#         for f in files:
#             try:
#                 total += store.add(f.name, f.getvalue())
#             except Exception as e:
#                 st.error(f"{f.name}: {e}")
#         st.success(f"Indexed chunks: {total}")
# 
#     st.subheader("2) Ask a question")
#     qcol1, qcol2, qcol3 = st.columns([3,1,1])
#     with qcol1:
#         question = st.text_input("Your question", placeholder="e.g., What is the renewal date and notice period?")
#     with qcol2:
#         top_k = st.number_input("Top-K chunks", min_value=1, max_value=20, value=TOP_K_CHUNKS, step=1)
#     with qcol3:
#         top_sents = st.number_input("Sentences in answer", min_value=1, max_value=10, value=TOP_SENTENCES, step=1)
# 
#     if st.button("Answer"):
#         results = store.search(question, top_k=top_k)
#         out = answer_with_sentences(question, results, top_sentences=top_sents)
#         st.markdown("### Answer")
#         st.write(out["answer"] or "_No answer_")
#         st.markdown("### Citations")
#         if out["citations"]:
#             for citem in out["citations"]:
#                 page = f"(p. {citem['page']})" if citem.get("page") else ""
#                 st.markdown(f"- **{citem['filename']}** {page} — score `{citem['score']:.3f}`\n\n> {citem['preview']}")
#         else:
#             st.write("_No citations_")
#         export_name = f"answer_{uuid.uuid4().hex[:8]}.pdf"
#         export_path = DATA_DIR / ws / export_name
#         with open(export_path, "wb") as fh:
#             # write the PDF export
#             export_answer_pdf(export_path, question, out["answer"], out["citations"], ws)
#         with open(export_path, "rb") as fh:
#             st.download_button("⬇️ Download answer as PDF", data=fh, file_name=export_name, mime="application/pdf")
# 
#     st.caption("Tip: create one workspace per client. Data is stored under DATA_DIR/workspace.")
# 
# # ----------------------------
# # PDF Export function (placed last so it's defined before use)
# # ----------------------------
# def export_answer_pdf(path: Path, question: str, answer: str, citations: List[Dict], workspace: str):
#     c = canvas.Canvas(str(path), pagesize=letter)
#     width, height = letter
#     margin = 0.75 * inch
#     x = margin
#     y = height - margin
#     def write_line(text, size=11, leading=14):
#         nonlocal y
#         c.setFont("Helvetica", size)
#         for line in text.split("\n"):
#             c.drawString(x, y, line)
#             y -= leading
#             if y < margin:
#                 c.showPage()
#                 y = height - margin
#                 c.setFont("Helvetica", size)
#     import textwrap
#     # header
#     c.setTitle("Knowledge Assistant — Answer Export")
#     write_line(f"{APP_TITLE} — Export", size=14, leading=18)
#     write_line(f"Workspace: {workspace}", size=10, leading=14)
#     write_line("")
#     # question + answer
#     write_line("Question:", size=12)
#     write_line(question)
#     write_line("")
#     write_line("Answer:", size=12)
#     for chunk in textwrap.wrap(answer, width=100):
#         write_line(chunk)
#     # citations
#     write_line("")
#     write_line("Citations:", size=12)
#     for citem in citations:
#         page = f"(p. {citem['page']})" if citem.get("page") else ""
#         write_line(f"- {citem['filename']} {page} — score {citem['score']:.3f}")
#         for chunk in textwrap.wrap(citem["preview"], width=100):
#             write_line(f"  {chunk}")
#     c.save()
# 
# if __name__ == "__main__":
#     main()
#

import os

# 🔐 Set the password for your app
# 👉 Change "demo123" to anything you like
os.environ["APP_PASSWORD"] = "demo123"
print("Password set! Use this in the login form: demo123")

# 📂 (Optional) Persist data to Google Drive so workspaces aren’t lost
# Uncomment these lines if you want persistence across Colab runs:
# from google.colab import drive
# drive.mount('/content/drive')
# os.environ["DATA_DIR"] = "/content/drive/MyDrive/knowledge_assistant_data"
# print("Data will be saved in Google Drive.")

# ---- New Cell 4: start Streamlit and create a public URL (robust) ----
import os, re, time, shutil, subprocess, textwrap

def ensure_cloudflared():
    """Install cloudflared if missing; fall back to downloading the binary."""
    if shutil.which("cloudflared"):
        return
    try:
        subprocess.check_call(["pip", "install", "-q", "cloudflared"])
    except Exception:
        pass
    if not shutil.which("cloudflared"):
        # Fallback: download the binary directly and put it on PATH
        cmd = textwrap.dedent("""
            set -e
            wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared
            chmod +x /usr/local/bin/cloudflared
        """).strip()
        subprocess.check_call(["bash", "-lc", cmd])

def start_streamlit():
    """Start Streamlit in headless mode on port 8501."""
    # Kill any previous Streamlit (optional)
    subprocess.run(["bash", "-lc", "pkill -f 'streamlit run app.py' || true"])
    return subprocess.Popen(
        ["streamlit", "run", "app.py", "--server.port", "8501", "--server.headless", "true"],
        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
    )

def start_tunnel():
    """Start cloudflared and return the public URL."""
    proc = subprocess.Popen(
        ["cloudflared", "tunnel", "--url", "http://localhost:8501", "--no-autoupdate"],
        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
    )
    public_url = None
    # Parse output lines until the URL appears
    for line in proc.stdout:
        if "trycloudflare.com" in line:
            m = re.search(r"https://[a-z0-9-]+\.trycloudflare\.com", line)
            if m:
                public_url = m.group(0)
                print("\n🔗 Your app is live here:\n", public_url)
                print("\n(Keep this cell running while you use the app.)")
                break
    return proc, public_url

# 1) Make sure cloudflared is available
ensure_cloudflared()

# 2) Start Streamlit
print("Starting Streamlit…")
st_proc = start_streamlit()
time.sleep(4)  # give Streamlit a moment to boot

# 3) Start tunnel and print URL
print("Starting Cloudflare tunnel…")
tunnel_proc, url = start_tunnel()
if not url:
    print("Couldn't detect the public URL yet. If it doesn't appear after ~10s, re-run this cell.")

from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas

pdf_path = "/content/sample_contract.pdf"
c = canvas.Canvas(pdf_path, pagesize=letter)
w,h = letter
c.setFont("Helvetica", 12)
c.drawString(72, h-72, "ACME SaaS Agreement")
c.drawString(72, h-100, "This Agreement is entered into on 1 Jan 2024.")
c.drawString(72, h-120, "The service will automatically renew on 31 Dec 2025.")
c.drawString(72, h-140, "The notice period for termination is 30 days before renewal.")
c.drawString(72, h-160, "Early termination is allowed for material breach.")
c.save()
print("Created:", pdf_path)
